---
layout: post
title: Transformer Quantization
category: notes
---

## Papers
Understanding and Overcoming the Challenges of Efficient Transformer Quantization (Qualcomm AI Research) [arXiv'21](https://arxiv.org/abs/2109.12948)

VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer [arXiv'22](https://arxiv.org/abs/2201.06618)

Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing [arxiv'23](https://arxiv.org/abs/2306.12929)

A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE [arXiv'24](https://arxiv.org/abs/2401.02721)

FrameQuant: Flexible Low-Bit Quantization for Transformers [arXiv'24](https://arxiv.org/abs/2403.06082)

Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization [arXiv'22](https://arxiv.org/abs/2208.05163)

Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization [glsvlsi'21](https://dl.acm.org/doi/abs/10.1145/3453688.3461739)