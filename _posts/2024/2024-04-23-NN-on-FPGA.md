---
layout: post
title: Employment of NN on FPGA
---

* TOC
{:toc}

## Background
### Number Representation
**Floating point number**

Floating point representation helps in representing large numbers and they may provide precision, but their implementation better and manipulation is difficult.
Again they consume so much resources, you won't be able to implement more than a few tens of neuron on a platform like Zedboard.

So we will be going with fixed point representation. Since the input values used in NNs are generally normalized (between 0 to 1 or -1 to 1), there won't be an issue of not able to represent large numbers.

**Fixed point number**

In this project, 15-bit fixed number is chosen as representation with 5 bits representing integer part, 10 bits representing fractional part.

For example, the representation of 15.84 is

01111  1101011100   15  84

But .11011011100 is 0.83984375 not 0.84. Thus it introduces an error of 0.00015625. That is less than 1.9x10-4 % but if the error accumulates, will severely affect the performance.

So instead of 15 bit representation, if we decide to use 10 bit representation with 5 bits for integer and 5 bits for fractional, 15.84 will be 0111111010 which is actually 15.8125. Here error is 0.0275, which is significantly higher than the previous one 

So by fixing the number of bits to represent, we do a tradeoff between accuracy and resource utilization.

**Signed-magnitude representation**

Here also you can choose between two representations, either signed-magnitude representation or 2's complement representation.
In signed magnitude representation, the MSB represents the sign. O if positive, 1 if negative.
Then as usual n bits will represent integer part and m bits will represent fractional part.

Eg: 0  11111101011100 represents +15.83984375

and 1  11111101011100 represents -15.83984375

Sign magnitude representation is very easy for us to
decode. But it has several drawbacks such as 2 representations for 0 (+0 and -0) and adding two sign magnitude numbers do not guarantee a sign magnitude result.

**2's complement representation**

Because of this 2's complement representation is
introduced:

Eg: 0  11111101011100 represents +15.83984375

and 1  00000010100100 represents -15.83984375

To find the 2's complement, you find 1's complement of the positive number (you can forget about the position of decimal point) and add 1 to it.

For addition (subtraction), 2's complement representation is very efficient since the result will be also in 2's complement representation. But for multiplication, it doesn't directly work in that way. For multiplication, sign magnitude might be more efficient.

### Activation Functions
Many neural networks use non-linear functions such as sigmoid $$\text{sigmoid}(x) = \frac{1}{1 + e^{-x}}$$ or hyperbolic tangent $$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$ as activation functions.

<!-- $$
\text{sigmoid}(x) = \frac{1}{1 + e^{-x}}
$$ -->

Again building digital circuits generating these functions are very challenging and they will be very resource intensive.

Hence, we generally pre-calculate their values (since we will be aware of range of x, the input) and store in a ROM. These ROMs will be also called Look Up Tables (LUTs).

Don't confuse with FPGA basic building block LUT. These LUT ROMs are built either using block RAMs or distributed RAMs (FFs and LUTs).

### IP Cores
We generally won't instantiate Xilinx IP cores (RAMs, DSP slices etc.) when building neural networks.
This will bring more flexibility to the design.

If you are using IP cores for example to design a ROM for activation function and if you decide to change the size of the ROM, you won't be able to do this by just changing a parameter. You will have to do it though Xilinx IP core generator. But these IP cores are highly efficient in implementation. For example BlockRAMs are much efficient than istributed RAMs when making ROMs.

## Implementation
![neuron](../../../../public/images/posts/2024/2024-04-23-NN-on-FPGA/neuron.pdf){: style="margin: 0 auto;"}
<div class="caption">
  <a href="https://www.youtube.com/watch?v=rw_JITpbh3k&list=PLJePd8QU_LYKZwJnByZ8FHDg5l1rXtcIq&index=1">NN on FPGA</a>
</div>