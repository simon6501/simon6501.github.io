---
layout: page
title: Notes
---

<p class="message">
  Stay tuned for updates! ðŸ˜€ðŸ˜‰
</p>

## Paper notes

Certainly! Here's your provided information formatted in Markdown style:

### Survey of Multi-modality Large Language Models:
- A Survey on Multimodal Large Language Models
- **Authors:** Tencent YouTu Lab
- **Link:** [arXiv'24](https://arxiv.org/abs/2306.13549)

### Transformer Hardware Accelerator:
- Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs
- **Authors:** OPPO AI Center
- **Link:** [arXiv'24](https://arxiv.org/abs/2403.20041)
---
- Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer
- **Link:** [SOCC'20](https://ieeexplore.ieee.org/abstract/document/9524802?casa_token=waZ1VVnVLHsAAAAA:Wm-pqhJHcrQ1SEubirzdw1WsjJyY9sbh2CNU8kP9LyS_bI1Qx6HRAFsxxdfyXNCWKcUG0rgHxg)
---
- TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer
- **Link:** [HPCA'22](https://ieeexplore.ieee.org/abstract/document/9773212?casa_token=LjFoEmvTZk8AAAAA:__alwZqW1r5yDLnv8wfX3_F5EvDJxnzkRtnJcWGFeHkm0202_j5La2jAeO8rTJW2yng8GNroqA)
---
- TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer
- **Link:** [ICCAD'21](https://ieeexplore.ieee.org/abstract/document/9643586?casa_token=2d_K8HXHBCsAAAAA:Db1BdFX8JPBQB53rIQjuu2dtmBaxLvoQMiISFyHu19vxvgcRNXdFTKgaKZghaOA3c95dXIcYNQ)
---
- A Survey on Transformer Compression
- **Link:** [arXiv'24](https://arxiv.org/html/2402.05964v1)
---
- An Efficient Hardware Accelerator for Sparse Transformer Neural Networks
- **Link:** [ISCAS'22](https://ieeexplore.ieee.org/abstract/document/9937659?casa_token=GU-_OSiD3EkAAAAA:seTGrT2HRPaad8VXDd7TWvp0FkeSqURil1MCj8xkaEXxWgjqT3dRRVchy08jJlofdL5zm_NCOw)
---
- ViA: A Novel Vision-Transformer Accelerator Based on FPGA 
- **Link:** [Trans CDICS'22](https://ieeexplore.ieee.org/abstract/document/9925700?casa_token=W3nv-Glo8ycAAAAA:VXPr0pn1PiJGKR-8PpvLdLoAYJs7GK1pEyNm6tDXcH8JyrFUn9EjTcgqg9I1CzCXlHRUEdEIeQ)
---
- Hardware Acceleration of Transformer Networks using FPGAs
- **Link:** [PACET'22](https://ieeexplore.ieee.org/abstract/document/9976354)
---
- Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning
- **Link:** [ISQED'21](https://ieeexplore.ieee.org/abstract/document/9424344?casa_token=OE8jWe4D-wcAAAAA:lrw52KTDDBOeCqmtehQVndmbu0L2TE7EoleIaIpy3Oe9__eCqErc2VDLQcmlLZefg0RjfvH6TA)
---
- A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining
- **Link:** [DAC'22](https://dl.acm.org/doi/pdf/10.1145/3489517.3530585)

### Large Language Model Hardware Accelerator:
- A Comprehensive Evaluation of FPGA-Based Spatial Acceleration of LLMs
- **Link:** [FPGA'24](https://dl.acm.org/doi/abs/10.1145/3626202.3637600)
---
- Invited Paper: Software/Hardware Co-design for LLM and Its Application for Design Verification
- **Link:** [ASP-DAC'24](https://ieeexplore.ieee.org/abstract/document/10473893?casa_token=jtqlhEEQSUgAAAAA:nEozyYx6DDfeQL_nR60PH-FTIRXJgiXrf-OW6qjqOLjD5uhvnzwiiGNnJQ2BTY667W-gwsjTEQ)
---
- The Breakthrough Memory Solutions for Improved Performance on LLM Inference
- **Authors:** Samsung Electronics
- **Link:** [IEEE Micro Early'24](https://ieeexplore.ieee.org/abstract/document/10477465?casa_token=HIWBOe-DFCQAAAAA:tEQsY8r6J8IuhRbLwkHiGALzRfDzzmnI8rfQGitDZANU87V-ZLSCJxU9qoGJIOnQRN1V-wKUTg)
---
- OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization
- **Link:** [ISCA'23](https://dl.acm.org/doi/abs/10.1145/3579371.3589038?casa_token=oC0Xzyuq1qsAAAAA:YUKnbev78hWOmA-aDpn5ql0ye7UWCwKsuMPs5M5c13Vc3eo3MvdbSIxX7FCgzOOdW1puKMpuuxhevA)
---
- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
- **Link:** [arXiv'23](https://arxiv.org/abs/2306.00978)

### Deep Learning & Hardware:
- Hardware Accelerator Design for Sparse DNN Inference and Training: A Tutorial
- **Authors:** NJU
- **Link:** [Trans Circuits and Systems'24](https://ieeexplore.ieee.org/abstract/document/10365687?casa_token=-x1VBz_dINcAAAAA:32cigMkZh4_XTYyq_f7q2xELLdYEeywsEK1oqvxVGutubXuaLGAQ-NLvYndiJ1CSs6JZE1-2dA)
